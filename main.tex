%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
% ref packages
%\usepackage{nameref}
%\usepackage{varioref}
% \usepackage{hyperref}
\usepackage[hyphens,spaces,obeyspaces]{url} % Fixes long URLs
\usepackage{hyperref} % Let URLs be clickable and lead to the website

% \usepackage{enumitem} % Let enumerations start with the alphabet
\usepackage{graphicx}
\usepackage{floatrow}
\usepackage{float}
\usepackage{aliascnt}
\usepackage{multirow} % Tables
\usepackage{array}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\usepackage{authblk} % For the author section 
\usepackage{caption} % advanced captions
%Mathebib
\RequirePackage{amsmath}
\RequirePackage{amssymb}
\usepackage[
style=ieee, % Zitierstil
isbn=false,
doi=false,
url=false,
pagetracker=true,
%autocite=inline,  % regelt Aussehen für \autocite (inline=\parancite)
block=space,               
backref=false,
backrefstyle=three+,
date=year,
backend=biber
]{biblatex}
\addbibresource{literature.bib}
\renewcommand*{\bibfont}{\small}

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\title{\LARGE \bf
Current Trends in Games Technologies: Upsampling algorithms
}

\author{Tobias Brandner}

\affil{Julius-Maximilians University \\
        Würzburg, Germany \\
        tobias.brandner@stud-mail.uni-wuerzburg.de}

%\author{Huibert Kwakernaak$^{1}$ and Pradeep Misra$^{2}$% <-this % stops a space
%\thanks{*This work was not supported by any organization}% <-this % stops a space
%\thanks{$^{1}$H. Kwakernaak is with Faculty of Electrical Engineering, Mathematics and Computer Science,
    %    University of Twente, 7500 AE Enschede, The Netherlands
     %   {\tt\small h.kwakernaak at papercept.net}}%
%\thanks{$^{2}$P. Misra is with the Department of Electrical Engineering, Wright State University,
      %  Dayton, OH 45435, USA
       % {\tt\small p.misra at ieee.org}}%
%}


\begin{document}
\maketitle
\thispagestyle{empty}
\pagestyle{empty}

\BiblatexSplitbibDefernumbersWarningOff

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

With the increasing complexity of games and the cost for computational demanding features like ray tracing, the need for image upsampling technologies has grown. 
4 prominent methods are Deep Learning Super Sampling (DLSS), FidelityFX Super Resolution (FSR), Xe Super Sampling (XeSS) and Temporal Super Resolution (TSR), which reduce computation costs by rendering the game in a lower resolution and later upsample it to the target resolution. 
DLSS and XeSS utilizes machine learning approaches, while FSR employs traditional methods. 
The paper evaluates these upsampling technologies in terms of their performance and image quality, considering benchmarks and quality metrics such as PSNR, SSIM, and LPIPS. 
DLSS proves to be the most effective upsampling technology, providing significant performance boosts and stable image quality. 
Recent developments include DLSS 3 and FSR 3, which generate entire frames between real images, dramatically increasing frame rate but introducing input lag.
As well as DLSS 3.5 - ray reconstruction, which further improves image quality for reflections, and AMD's Fluid Motion Frame (AFMF), which enables driver-level frame generation.

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

As games become more sophisticated by incorporating computational demanding features such as ray tracing, a method of calculating illumination by tracking virtual photons, 
which are reflected and scattered by surfaces \cite{ray_tracing}, the need for image upsampling technologies is increasing.
Deep Learning Super Sampling (DLSS) \cite{dlss} and FidelityFX Super Resolution (FSR) \cite{fsr} are two different methods to reduce computation cost 
by rendering the game in a lower resolution and later upsample them to the higher target resolution. 
While FSR uses traditional methods, DLSS uses machine learning approaches to achieve upsampling.\\
The game Cyperpunk \cite{Cyperpunk}, for example, recently received an experimental feature that has been developed in cooperation with Nvidia\footnote{\url{https://www.nvidia.com/de-de/}} 
called Ray Tracing: Overdrive \cite{ray_tracing_overdrive}, a path tracer that uses all light sources in the scene to calculate reflections for the current rendered frame \cite{Shirley2020RTW1}.
The calculation for this type of ray tracing is so demanding that an RTX 4090, 
one of the most powerful General Processing Units (GPU) \cite{4k_gpu_bench}, renders the game in 4k resolution at only 18 frames per second (fps) at average. 
This compares to an average of 48 fps without ray tracing. Using DLSS 2 the average fps increases to 59 fps, and with NVidia's latest upsampling technology DLSS 3 an average of 95 fps is achieved \cite{digital_foundry_ray}. 
A frame rate that makes the game fluid and responsive to play \cite{goodFPS}.
On the other hand, upsampling technologies can also be used to support weaker hardware such as older GPUs or mobile hardware \cite{dong2022rendersr}.
The Legend of Zelda: Tears of the Kingdom \cite{ZeldaTotK}, for example, is rendered natively in 720p resolution on the Nintendo Switch\footnote{\url{https://www.nintendo.com/switch/}} when played in handheld mode, 
but when docked to the station it is rendered in btw. 720p and 900p at 30 fps and further upscaled with FSR 1.0 to reach 1080p \cite{digital_foundry_zelda_fsr}. \\
The following section \ref{Sec:RelatedWork} explains the core ideas behind traditional and machine upsampling techniques.
Section \ref{Sec:Current} examines the state of the art of upsampling technologies used in games.
Section \ref{Sec:Evaluation} evaluates these upsampling technologies against some benchmarks, including recent game titles and their respective image quality.
Finally, section \ref{Sec:ConcFuture} summarizes the results and explores future trends.

\section{Background}
\label{Sec:RelatedWork}
Image Upsampling/Upscaling is the process of of creating a high resolution (HR) image from a low resolution (LR) image.
This section explains some of the most common ideas, as seen in figure \ref{fig:upsampling_methods}, when it comes to upsampling \cite{wang2022interpolation}\cite{wang2020deep}.
Starting with more traditional linear interpolation methods to cutting edge deep learning approaches. 

\begin{figure}[!ht]
        \caption{Hierachically-structured taxonomy of this paper.
        We review two methods for image upsampling: 
        Interpolation, e.g the linear Bicubic interpolation, and Deep Learning, consisting of supervised and unsupervised network models.}
        \centering
        \includegraphics[width=0.7\textwidth]{images/interpol.jpg}
        \label{fig:upsampling_methods}
    \end{figure}

\subsection{Interpolation}
\label{subsec:traditional}
Interpolation is the method of constructing new data points within the range of a discrete set of known data points.
In the context of images, this means that we need to "guess" the color information of the newly created pixels \cite{image_interpolation_def}.
One example: If we have an LR image of 720p (1280x720) and we want to double the resolution to 1440p (2560x1440), 
we need to create a new pixel for each pixel of our input image and interpolate their color values.

\subsubsection{Linear Interpolation}

Nearest neighbor interpolation is one of the simplest and fastest interpolation algorithms, but it provides poor image quality. 
The idea is that each interpolated pixel gets the same color value as its nearest neighbor. 
Therefore we need to identify the interpolated pixel's coordinate and scale it based on the ratio between the original image size and the target image size.
(coming back to our example 720p to 1440p, it would be 0.5) and chose the pixel in the original image with the nearest coordinate \cite{nearest_n}.
Bi-linear interpolation, as the name suggests, goes one step further and interpolates in two directions. 
Thus, the color values of the 4 nearest neighbors are taken and their average is used \cite{bilinear}.
Bicubic interpolation is the most common and computationally intensive linear method we discuss. 
It takes a 4x4 grid of neighboring pixels and weights them based on their distance (closer pixel values are weighted higher) \cite{bicubic}. 
Resulting in better image quality overall, except for some edge cases (pun intended).

\subsubsection{Nonlinear Interpolation}

Linear interpolation methods work well for upsampling pixel areas, but fail at edges, resulting in blurring.
For these cases, nonlinear interpolation methods such as unsharp masking can be used. 
Unsharp masking creates a blurred version of the original image and subtracts it from the original, creating an image mask that contains only the edges.
This mask can then be used to resharpen the upsampled image, by selectively increasing the contrast along the edges \cite{deng2010generalized} \cite{unmask}.
Another method is Laplacian sharpening, which uses the Laplacian operator to highlight areas of rapid intensity change (edges), and then reapplies it to the image, again resulting in sharper edges \cite{wang2022interpolation}.\\
Linear and nonlinear interpolation methods can be combined, as seen in figure \ref{fig:edge_det}, by detecting when to use a no-/linear method to obtain a high quality sampled image.

\begin{figure}[!ht]
        \caption{When upsampling an image from a low-resolution image, edges are detected to determine which type of interpolation method to use.
                If an edge is detected, a nonlinear interpolation method is used to sharpen the edge of the generated high-resolution image. 
                Otherwise, a linear interpolation method is used to fill areas in the generated image.}
        \centering
        \includegraphics[width=0.6\textwidth]{images/edgedet.jpg}
        \label{fig:edge_det}
    \end{figure}

\subsection{Deep Learning}

%Supervised models (LR images and corresponding HR images for training) vs Unsupervised \\
Image Super Resolution (SR) is a special form of image upsampling where more complex and advanced algorithms, e.g. deep neural network models, are used to generate an HR image from an LR image. 
In general, each neural network processes an input, in our case an LR image, with multiple layers (the more layers, the deeper the network) and produces an output, in our case an HR image.
The input, i.e. the image, is first converted into a tensor, a multidimensional array consisting of numbers representing the color values of each pixel. 
The layers then perform various mathematical operations based on their own parameters, i.e. weights to learn features like detecting pixel corresponding to edges.
The output of the network produces a loss that shows how well the network performed its task. 
This loss is then backprobagated to the various layers to update their parameters and adjust their weights to hopefully produce a better/more accurate output.
This process is repeated many times and is called training of the network \cite{goodfellow2016deep}.
After the network is trained, it can be used to generate HR images.\\
Since this is a very complex topic and there are many different neural network architectures, we will focus on some key architectures.

\subsubsection{A supervised method}
Supervised neural networks are networks whose input and output are carefully prepared and labeled, 
i.e., each LR image that is fed into the network has a corresponding HR image against which the output of the network can be compared.
%CNN erklären
The most common supervised neural network is the Covolution Neural Network (CNN).
It uses convolutional layers to extract features from training data. 
Convolutional layers perform a mathematical operation called convolution, which multiplies two higher-dimensional arrays to compute a new value (effectively downsampling our image).
The inverse of this operation, deconvolution or transposed convolution, can be used to sample the image back up.
\textcite{Gao2020PixelTC} has shown that their network PixelTCL can effectively overcome checkerboard artifacts, resulting in better spatial features such as edges and shapes.
\\
% GAN erklären
Another model is the Generative Adversial Network (GAN), which consists of two main components: a generator and a discriminator.
Both are trained simultaneously: 
The generator has the task of generating an image that the discriminator takes as input.
The discriminator receives a mixture of real and fake images from the generator, and its task is to discriminate between them.
GAN models have proven to be very successful when it comes to generating HR images \cite{Sharma2020HighresolutionID}.


\subsubsection{A Unsupervised method}
% UNet erkläten
% Figure größer machen -> maybe think about making it go over both columns
\begin{figure*}[!ht]
    \caption{An example of a UNet network architecture. 
    The input gets downsampled with intermediate steps and then again upsampled, forming a U-like structure.}
    \centering
    \includegraphics[width=0.8\textwidth]{images/U-NetExample.png}
    \label{fig:unet}
\end{figure*}
The U-Net architecture can be used for both supervised and unsupervised training. 
In this architecture, first the input is downsampled and then later upsampled with intermediate steps, creating a U-like structure, as seen in figure \ref{fig:unet}.
The unique feature of this network architecture is that each upsampling step is influenced not only by the previous step, but also by the same layer, so that more unique features can be learned \cite{Sharma2022ADL}. \\
The Convolutional Auto-Encoder is another unsupervised method. 
An autoencoder is a type of neural network that aims to learn a compressed representation or encoding of the input data.
It consists of an encoder network that maps the input image to a lower dimensional representation, 
a bottleneck layer that helps compress the extracted features into a smaller vector representation, 
and a decoder that gradually converts the compressed image into an upsampled image.
In the case of a convolutional autoencoder, the encoder and decoder networks consist of convolutional layers instead of fully connected layers \cite{conv_autoE}.
Convolutional autoencoders have proven effective in capturing spatial information and extracting features from images for use in image upsampling \cite{liu2021variational}.

\subsection{Anti-aliasing}

Anti-aliasing is a technique for eliminating the aliasing effect. 
Aliasing describes the appearance of stair-stepped lines or jagged edges in a rasterized image (image rendered with pixels), as seen in figure \ref{fig:antiAliasing}.
Aliasing typically occurs when displaying HR images on a lower resolution system, for example when smooth and continuous curves are rasterized \cite{antialiasing_def}.

\begin{figure}[!ht]
        \caption{An example of a) anti-aliasing having sharp edges, and b) aliasing having stair-stepped lines.}
        \centering
        \includegraphics[width=1.0\textwidth]{images/aliasing.jpg}
        \label{fig:antiAliasing}
    \end{figure}

The idea behind anti-aliasing is very similar to upsampling methods, only their use case differs, as interpolation is applied from LR images to HR images, while anti-aliasing is applied from HR images to LR images.
Anti-aliasing is usually needed for a large screen size with a relatively low screen resolution.\\
An example would be Super Sampling Anti-Aliasing (SSAA), which renders and downsamples the image at a higher resolution to produce a clearer and sharper image. 
Or Multi-Sample Anti-Aliasing (MSAA), which takes multiple samples per pixel during the rendering process. 
Instead of rendering a single color for each pixel, MSAA calculates the average color value based on the samples within the pixel range. 
By averaging multiple samples, MSAA can smooth edges and reduce the jagged appearance.
But these approaches are quite computationally expensive so using Morphological Anti-Aliasing (MLAA), a rule based pixel blending anti-aliasing method which added blurring to the image, gained a lot of popularity. 
Born from MLAA was Fast Approximate Anit-Aliasing (FAA) and Subpixel Methodological Anti-Aliasing (SMAA), which reduce the blur to a minimum, resulting in a overall better and sharper image.
The last Anti-Aliasing method is Temporal Anti-Aliasing (TAA), a complex and intensive method that uses a temporal component (frames that were rendered before the current generated image). 
TAA helps in smoothing artifacts from rash movement \cite{antialiasing_types} \cite{gu2022super} \cite{stuttgart_boy}.\\
In the context of video games, upsampling and anti-aliasing are often combined to provide a smooth and good-looking visual experience.

\subsection{Quality Metrics}

Measuring the quality of created images is not a trivial task, as the human eye is very perceptive when it comes to detecting subtle differences that make images realistic or unrealistic.
The most common objective quality measurement is the peak signal-to-noise ratio (PSNR). 
It compares two images pixel for pixel by calculating the mean squared error (MSE) between them, summing them up, and putting it on a logarithmic scale, resulting in a comparable value.
Because PSNR only considers the MSE at the pixel level, i.e., the differences between the corresponding pixels, and not the visual perception, it is often a poor representation of the overall quality of the reconstruction.
Structural similarity (SSIM) attempts to compensate for this shortcoming by measuring the structural similarity between two images. 
This is based on the assumption that the human eye is more sensitive to changes in image structures.
SSIM measures luminance, contrast, and structure independently by estimating luminance and contrast as the mean and image intensity as the standard deviation.
They are combined in pixel windows and summed to calculate a final metric value. 
Learned Perceptual Image Patch Similarity (LPIPS) is another quality metric, that uses a deep neural network to calculate the similarity between their respective feature representations obtained from the trained network.\\
Those objective metrics may not indicate good overall image quality. Therefore, a subjective metric, such as Mean Opinion Score (MOS), may also be considered.
MOS allows test subjects to rate actual image quality, usually by assigning a simple score from 1 (poor) to 5 (good).
The MOS score is calculated by taking the average of all the scores. 
The MOS score is inferior to the objective scores when it comes to comparability, but can always serve as a good basis when objective measures provide varying results \cite{wang2020deep} \cite{Soufi2022BenchmarkOD}.


\section{Current Upsampling Technologies}
\label{Sec:Current}

\begin{table*}[ht]
    \centering
    \caption{FSR, DLSS, and XeSS have different quality modes that can be set. These upsampling modes usually include:
        a) A very high setting (best image quality with a scaling factor of 1.3).
        b) A high setting (good overall image quality with a scaling factor of 1.5).
        c) A medium setting (mixed image quality with a scaling factor of 1.7).
        d) A low setting (mediocre image quality with a scaling factor of 2.0).
        e) A very low setting (worst image quality with a scaling factor of 4.0)
    The higher the scaling factor, the worse the upscaling becomes!}
    \label{tab:qualityModes}
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Quality Mode} & \textbf{Scale Factor} & \textbf{Input Resolution} & \textbf{Output Resolution} \\
    \hline
    Very High   & 1.3x                          & 1477 x 831  & 1920 x 1080 (FHD) \\
                & (77\% screen resolution)      & 1970 x 1108 & 2560 x 1440 (WQHD) \\
                &                               & 2954 x 1662 & 3840 x 2160 (4k) \\
    \hline
    High        & 1.5x                          & 1280 x 720  & 1920 x 1080 \\
                & (67\% screen resolution)      & 1706 x 960  & 2560 x 1440 \\
                &                               & 2560 x 1440 & 3840 x 2160 \\
    \hline
    Medium      & 1.7x                          & 1129 x 635  & 1920 x 1080 \\
                & (59\% screen resolution)      & 1506 x 847  & 2560 x 1440 \\
                &                               & 2259 x 1270 & 3840 x 2160 \\
    \hline
    Low         & 2.0x                          & 960 x 540   & 1920 x 1080 \\
                & (50\% screen resolution)      & 1280 x 720  & 2560 x 1440 \\
                &                               & 1920 x 1080 & 3840 x 2160 \\
    \hline
    Very Low    & 4.0x                          & 960 x 540   & 3840 x 2160 \\
                & (25\% screen resolution)      & 1920 x 1080 & 7680 x 4320 (8k) \\
    \hline
    \end{tabular}
\end{table*}

So far, we have only dealt with the basics of upsampling techniques, which mostly focused on a static context (we did not care about how long it takes our algorithm to generate the HR image).
We will now focus on the latest upsampling technologies used in modern games. Therefore, these algorithms need to upsample our image in a real-time context, e.g. upsample an image 60 times per second.
In particular, we study FSR from AMD and DLSS from Nvidia, 
Xe Super Sampling (XeSS) \cite{XeSS_intro} from Intel\footnote{\url{https://www.intel.com/content/www/us/en/homepage.html}} 
and Temporal Super Resolution (TSR) \cite{TSR_intro} from Epic Games\footnote{\url{https://www.epicgames.com/site/en-US/home/}} (which is used exclusively in the Unreal Engine\footnote{\url{https://www.unrealengine.com/en-US/}}). \\
All of these technologies typically use 5 different quality modes to upscale the LR image, as seen in table \ref{tab:qualityModes}. 
For example, the very high quality mode uses a factor of 1.3 to scale the input resolution from 2954 x 1662 to 3840 x 2160 (here only 77\% of the output resolution is actually rendered by the game).
Higher scaling factors result in worse image quality, but increase the performance rate of the respective upsampling algorithm.
All quality modes target output resolutions of FHD (1080p), WQHD (1440p), and 4k (2160p).
To include an upsampling technology inside the render process of a game engine, like Unreal or Unity\footnote{\url{https://unity.com/}}, 
the upsampling pass needs to be applied after the anti-aliasing, but before rendering post processing effects, especially those that add noise like film grain, and HUD elements, as seen in image \ref{fig:renderpass}.

\begin{figure}[!ht]
    \caption{To include an upsampling technique in the render pass of a game engine, 
    it should be used after anti-aliasing the image, but before rendering post process effects, 
    like film grain or chromatic abberation, and the HUD elements.}
    \centering
    \includegraphics[width=0.3\textwidth]{images/renderpass.jpg}
    \label{fig:renderpass}
\end{figure}

\subsection{FidelityFX Super Resolution (FSR)}

FSR is AMD's open source, cross platform (runs on all brands of GPUs) upsampling technology. It was first introduced as FSR 1.0 in June 2021.
FSR 1.0\footnote{\url{https://github.com/GPUOpen-Effects/FidelityFX-FSR}} only relies on traditional interpolation methods. In particular it uses two passes: 
\begin{enumerate}
    \item An upscaling pass called Edge-Adaptive Spatial Upsampling (EASU) that interpolates and analyses the input frame to detect gradient reverseals (essentially looking at how strongly neighboring pixel values differ). 
    \item A sharpening pass called Robust Contrast-Adaptive Sharpening (RCAS) to reapply the detected edges into the upsampled image. 
\end{enumerate}

For FSR 1.0 to work well, it needs anti-aliasing (in form of TAA or MSAA) as well as a negative Multum In Parvo (latin for many things in a small space) (MIP) mapping bias.
MIP maps are precomputed versions of textures at different resolutions, they are used to improve rendering performance and reduce visual artifacts, like aliasing, by chosing the appropriate level of detail (LOD) of a texture for a given pixel \cite{MIP_map}.
The game engine automatically selects the MIP mapping. With a bias we can change the LOD of the texture. While a positive bias leads to a lower LOD and makes the texture appear blurry, a negative bias leads to a higher LOD and makes the texture sharper \cite{MIP_mapBias}.
In particular for FSR, different MIP biases are chosen depending on the quality level (-0.3 for very high to -1.0 for low).
As mentioned before, FSR needs to perform the upsampling in real time and therefore it is important to know how long the upsampling process takes.
Depending on the target resolution and GPU, upsampling takes less than 1 millisecond (ms) (4k with a 3080 takes 0.4 ms, with a 2060S it takes 1.0 ms).
For comparison, at a refresh rate of 60 fps, an image takes 16.6 ms to render. Therefore, the performance overhead of <=1ms is acceptable with FSR \cite{fsr1.0}.
Games can be either GPU-limited or CPU-limited, meaning that the game's frame rate depends either on how long it takes the game to render a frame (GPU limit) or how long it takes the game to compute a step in its logic (CPU limit) \cite{gpu-cpu-limit}.
Since upsampling techniques reduce the performance cost to the GPU, meaning when the game is GPU limited the performance overhead of the upsampling process when rendering the game at a lower resolution is far less than the overhead when rendering the game at a higher resolution, which results in a dramatic improvement in our frame rate!\\
FSR 2.x\footnote{\url{https://github.com/GPUOpen-Effects/FidelityFX-FSR2}} is AMD's latest upsampling technology and was released in May 2022.
It builds on FSR 1.0 and uses a temporal algorithm similar to TAA. The idea is to use motion vectors that describe how a game object moved from the previous frame to the current one.
This history data can then be used to increase the accuracy of the interpolated pixels by calculating where each pixel in the previous frame moved to.
In cases where the history data no longer correlates with the new input frame, temporal ghosting (a visual artifact best seen in fast-moving objects that trace themselves) occurs. 
Since FSR 2.x already uses something similar to TAA, an anti-aliasing image as input is not strictly necessary (the MIP bias remains the same).
Therefore, it is more computational demanding, which leads to a doubling of the performance overhead (4k with a RX 6800XT takes 1.1ms) \cite{fsr2.x}.

\subsection{Deep Learning Super Sampling (DLSS)}

DLSS is Nvidia's proprietary deep learning upsampling technology that only works on the company's RTX GPUs.
First introduced in 2018, it uses TAA for the LR image and then feeds it into a convolutional autoencoder network to get an upsampled HR image.
The network is trained by comparing its output to an even higher resolution image, e.g. the network upscales a 1080p image to 4k and compares it to a 16k base image \cite{dlss_slide_1}.
The network can then be fine-tuned by optimizing for several losses, such as content loss and style loss.
The content loss measures the difference between the content of the base image and the up-sampled image.
It ensures that important content and structural details of the original image are captured.
Content loss is usually calculated using a distance metric such as MSE.
Style loss, also known as texture loss, focuses on capturing the style or artistic features of the base image.
Style is often represented by correlations of different feature vectors in the CNN, which are stored in the Gram matrix.
Style loss is calculated by comparing the Gram matrices of the feature vectors of the original image and the upscaled image \cite{liu2021variational}.\\
In 2020, Nvidia further improved DLSS to DLSS 2 by adding information about previous images in the form of motion vectors. 
This temporal feedback is then fed into the network in addition to the TAA input image to further improve the network's output.
When integrating DLSS 2, it is recommended to use a MIP mapping bias based on the formula \ref{dlss_mip_bias} \cite{dlss_slide_2}.

\begin{equation}
    \log_2\left(\frac{\text{{renderWidth}}}{\text{{outputWidth}}}\right) - 1 + \text{{eps}}
    \label{dlss_mip_bias}
\end{equation}

A look at the official SDK\footnote{\url{https://github.com/NVIDIA/DLSS}} of DLSS 2 shows that 6 different presets can be selected, which means that there are 6 different configurations for the parameters of the network. \\
DLSS 3 is Nvidia's latest upsampling technology, introduced in 2022, and only works on their 4000 generation GPUs.
Building on DLSS 2, DLSS 3 adds optical frame generation that not only allows an LR frame to be upsampled to an HR frame, but also creates an entirely new frame between the upsampled frames.
The DLSS 3 convolutional autoencoder requires four inputs - current and previous frames, an optical flow field, and game engine data such as motion vectors and depths.
The optical flow field is calculated by analyzing two consecutive frames in the game.
This makes it possible to track history data not only for game objects, as motion vectors do, but also for pixel-level information such as particles, reflections, shadows, and lighting.
Because DLSS 3 generates an entire fake frame between real frames (for which no game logic calculations are required), it reduces the performance overhead for the CPU.
When a game is CPU-limited, this behavior leads to an improvement in our frame rate!
While the fake frames are displayed, the game does not respond to the player's input.
This reduces the overall responsiveness of the game.
(Note: To solve this problem, Nvidia has introduced another technology called Refelex).

\subsection{Xe Super Sampling (XeSS)}

In September 2022, Intel introduced XeSS as its own deep learning upsampling technique.
The SDK\footnote{\url{https://github.com/intel/xess/tree/main}} to use their network model is open source and works for all GPU brands.
Unfortunately, at this point we can't say anything about the actual network architecture used.
According to the official XeSS developer's guide, the model takes as input: the previous and current frames (aliased), the motion vectors and the depth buffer.
XeSS uses a jitter offset to fight jagged edges, and takes a new subpixel position at each frame to ensure temporal convergence (to fight temporal artifacts such as ghosting or blurring).
Similar to FSR 2, XeSS performs its own anti-aliasing during upsampling.
Intel recommends a MIP mapping bias based on an analog formula \ref{dlss_mip_bias} used in DLSS for XeSS.
It is also recommended to use 1080p and above as input resolution for XeSS to work well.

\subsection{Temporal Super Resolution (TSR)}

In December 2020, Epic Games introduced Temporal Anti-Aliasing Upsampling (TAAU) as the first version of an upsampling technique. 
TAAU replaced TAA in Unreal Engine version 4.26 and combined anti-aliasing and upsampling in one step in the render chain.
Of course, it can only be used for anti-aliasing purposes if the input resolution is greater than or equal to the output resolution \cite{unreal_all}.
With the release of Unreal Engine 5.0 in April 2022, Epic Games further improved TAAU to TSR.
TSR is platform independent and works on GPUs of all brands, but is optimized for Playstation 5 and Xbox Series S/X consoles (the consoles' AMD GPUs).
Similar to FSR 2, DLSS 2 and XeSS, it uses previous frames and motion vectors to create an accurate upsampled frame. 
Unfortunately, again, we can't tell you what exact methods TSR uses.
Both interpolation and deep learning methods seem reasonable here.
Epic Games recommends using at least 1080p as the input resolution to get good image quality.
The screen resolution can be set up to 25\%, resulting in a maximum upscaling factor of 4.
TSR cost can be roughly estimated at 1.5 ms per frame.
MIP mapping bias is handled automatically by the engine.
If another upsampling technique (FSR, DLSS, XeSS) is enabled within the engine, TSR simply gets replaced \cite{TSR_intro}.\\

A full list of which game engines support which upsampling technique can be seen in \ref{tab:upscaling}.

\begin{table}[htbp]
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
    \multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{\textbf{Godot}} & \multicolumn{1}{c|}{\textbf{Unreal}} & \multicolumn{1}{c|}{\textbf{Unity}} \\
    \hline
    FSR 2 & \color{green}\checkmark & \color{blue}\checkmark & \color{blue}\checkmark**\\
    \hline
    DLSS 2 & \color{red}x & \color{blue}\checkmark & \color{green}\checkmark\\
    \hline
    XeSS & \color{red}x & \color{blue}\checkmark & \color{red}x\\
    \hline
    TSR & \color{red}x & \color{green}\checkmark & \color{red}x \\
    \hline
    FSR 3 & \color{red}x & \color{blue}\checkmark* & \color{red}x\\
    \hline
    DLSS 3 & \color{red}x & \color{blue}\checkmark & \color{green}\checkmark*\\
    \hline
    \end{tabular}
    \caption{Support of upsampling techniques in game engines. 
    Green means native support inside the engine itself, while blue means it is supported through a plugin. 
    (* announced or in development)
    (** no official plugin)
    \cite{fsr2.x_unreal} \cite{xess_unreal} \cite{dlss_unity} \cite{dlss_unreal} \cite{fsr2_godot} \cite{fsr2_unity}}
    \label{tab:upscaling}
  \end{table}


\section{Evaluation}
\label{Sec:Evaluation}
For evaluating the different upsampling technologies, we first focus on their boost in performance and second look into the generated image quality.

\subsection{Benchmarks}

To compare the performance increase of the different upsampling technologies, we use an example benchmark \ref{fig:comparison} of Cyperpunk 2077 in Raytracing Overdrive mode with a 4090.
The source \cite{bench1}\cite{bench2} for this benchmark used a strong CPU (i9 13900k and Ryzen 7700X), so the game is GPU-limited with this configuration. 
Besides the average fps, we also compare the 1\% lows of the game. 
1\% lows occur when the game is under a lot of pressure, i.e. when many objects and particle effects have to be rendered.
This makes it a good benchmark variable to capture microstutters that can be annoying for players.
\begin{figure}[!ht]
    \caption{Comparing the fps btw. the different upsampling technologies in their quality mode and Native 4k/WQHD in Cyperpunk 2077 with a RTX 4090.}
    \centering
    \includegraphics[width=1.0\textwidth]{images/comparison.png}
    \label{fig:comparison}
\end{figure}
The benchmark compares the average and 1\% lows when rendering the game in native 4k and WQHD with the upsampling technologies.
All upsampling technologies use the high quality mode, i.e. the input resolution is WQHD and the output resolution is 4k.
DLSS 2 and FSR 2 have a slight advantage over XeSS and TSR. Still, all upsampling technologies increase fps significantly. 
(Since TSR is a pure Unreal upsampling method, another source \cite{bench3} comparing DLSS, FSR and TSR in Ghostwire Tokyo \cite{Ghostwire} was used to estimate TSR performance for this benchmark).
DLSS 2, in particular, comes precariously close to the performance of the game's native rendering at the input resolution.
DLSS 3's frame generation results in another significant increase in fps, even compared to rendering the game in WQHD (about 60\%)!
This makes it the undisputed number one upsampling technology when it comes to increasing raw frame rate.

\subsection{Image Quality}

None of the four upsampling methods published an objective metric score for PSNR, SSIM, or LPIPS.
Looking at similar approaches as FSR \cite{wang2022interpolation} and DLSS \cite{liu2021variational}, we find that interpolation methods have a higher PSNR and SSIM value (a higher pixel-based accuracy),
but a lower LPIPS value (a lower human perception of image quality) compared to deep learning methods.
A look at an image quality comparison source (\cite{bench4}) shows that TSR and FSR have more problems with pixel flicker when rendering very fine geometries such as plants or grids, while DLSS provides the most stable image across multiple frames.
In general, lowering the quality modes for all upsampling technologies results in more visual artifacts, such as ghosting.

\section{Conclusion \& Future Work}
\label{Sec:ConcFuture}

Upsampling technologies can be based on either interpolation (FSR) or deep learning (DLSS).
Regardless of the method used, upsampling technologies have proven to increase game performance by rendering the game at a lower resolution (lifting GPU limits) or/and creating a frame between real frames (lifting CPU limits).
The freed up power can be used for new features, e.g. more accurate ray tracing, help older GPUs/CPUs to be usable longer or just to save energy.
At the moment, DLSS is proving to be the best upsampling technology in terms of performance and image quality.
Since the other upsampling technologies are newer, we can expect this gap to close in the future.
Right now, all vendors of upsampling technologies recommend an input resolution of at least 1080p to achieve acceptable image quality.
A rather high resolution for weak hardware, e.g. cell phones, to render.
In the future, it is expected that acceptable image quality can also be achieved with lower resolutions.\\
Regarding frame generation:
AMD recently released their first version of frame generation called FSR 3, which builds upon AMDs Fluid Motion Frames (AMFM).
Similiar to DLSS 3 AFMF creates an optical flow field to calculate the position of all objects in the scene.
AFMF is a driver based solution and can be used solely for frame generation (while FSR 3 always works with information of FSR 2 and therefore needs to be implemented in some way in the renderer of the game).
This allows AMFM to work in games which do not have any support of FSR and therefore can be used retrospective on older games.
At the moment AFMF is in a beta state and limited to games running with DirectX 11 and 12.
The generated frames lack important information about the game (like motion vectors) and therefore look in most cases very blurry \cite{afmf}.
FSR 3 however generates fake frames with acceptable level of detail, but at the moment struggles with frame pacing \cite{fsr3}.
Nvidia on the other hand recently revealed DLSS 3.5 - Ray Reconstruction, which enhances ray-traced images for intensive ray-tracing applications.
DLSS 3.5 is trained on 5 times the data of that of DLSS 3 with the aim to reduce denoiser interference, resulting in better ray tracing effects, such as reflections \cite{dlss_3.5}. 
Upsampling and generation techniques for images and frames is and remains a hot topic where we will see further improvements in detail quality in the future.

% \addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\printbibliography[
title={Papers},
notkeyword=game, notkeyword=online
]

\printbibliography[
title={Online},
keyword=online
]

\printbibliography[
title={Ludography},
keyword=game
]

%\printbibliography

% \begin{thebibliography}{99}

% \bibitem{c1} G. O. Young, ``Synthetic structure of industrial plastics (Book style with paper title and editor),'' 	in Plastics, 2nd ed. vol. 3, J. Peters, Ed.  New York: McGraw-Hill, 1964, pp. 15--64.
% \bibitem{c2} W.-K. Chen, Linear Networks and Systems (Book style).	Belmont, CA: Wadsworth, 1993, pp. 123--135.
% \bibitem{c3} H. Poor, An Introduction to Signal Detection and Estimation.   New York: Springer-Verlag, 1985, ch. 4.
% \bibitem{c4} B. Smith, ``An approach to graphs of linear forms (Unpublished work style),'' unpublished.
% \bibitem{c5} E. H. Miller, ``A note on reflector arrays (Periodical styleÑAccepted for publication),'' IEEE Trans. Antennas Propagat., to be publised.
% \bibitem{c6} J. Wang, ``Fundamentals of erbium-doped fiber amplifiers arrays (Periodical styleÑSubmitted for publication),'' IEEE J. Quantum Electron., submitted for publication.
% \bibitem{c7} C. J. Kaufman, Rocky Mountain Research Lab., Boulder, CO, private communication, May 1995.
% \bibitem{c8} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interfaces(Translation Journals style),'' IEEE Transl. J. Magn.Jpn., vol. 2, Aug. 1987, pp. 740--741 [Dig. 9th Annu. Conf. Magnetics Japan, 1982, p. 301].
% \bibitem{c9} M. Young, The Techincal Writers Handbook.  Mill Valley, CA: University Science, 1989.
% \bibitem{c10} J. U. Duncombe, ``Infrared navigationÑPart I: An assessment of feasibility (Periodical style),'' IEEE Trans. Electron Devices, vol. ED-11, pp. 34--39, Jan. 1959.
% \bibitem{c11} S. Chen, B. Mulgrew, and P. M. Grant, ``A clustering technique for digital communications channel equalization using radial basis function networks,'' IEEE Trans. Neural Networks, vol. 4, pp. 570--578, July 1993.
% \bibitem{c12} R. W. Lucky, ``Automatic equalization for digital communication,'' Bell Syst. Tech. J., vol. 44, no. 4, pp. 547--588, Apr. 1965.
% \bibitem{c13} S. P. Bingulac, ``On the compatibility of adaptive controllers (Published Conference Proceedings style),'' in Proc. 4th Annu. Allerton Conf. Circuits and Systems Theory, New York, 1994, pp. 8--16.
% \bibitem{c14} G. R. Faulhaber, ``Design of service systems with priority reservation,'' in Conf. Rec. 1995 IEEE Int. Conf. Communications, pp. 3--8.
% \bibitem{c15} W. D. Doyle, ``Magnetization reversal in films with biaxial anisotropy,'' in 1987 Proc. INTERMAG Conf., pp. 2.2-1--2.2-6.
% \bibitem{c16} G. W. Juette and L. E. Zeffanella, ``Radio noise currents n short sections on bundle conductors (Presented Conference Paper style),'' presented at the IEEE Summer power Meeting, Dallas, TX, June 22--27, 1990, Paper 90 SM 690-0 PWRS.
% \bibitem{c17} J. G. Kreifeldt, ``An analysis of surface-detected EMG as an amplitude-modulated noise,'' presented at the 1989 Int. Conf. Medicine and Biological Engineering, Chicago, IL.
% \bibitem{c18} J. Williams, ``Narrow-band analyzer (Thesis or Dissertation style),'' Ph.D. dissertation, Dept. Elect. Eng., Harvard Univ., Cambridge, MA, 1993. 
% \bibitem{c19} N. Kawasaki, ``Parametric study of thermal and chemical nonequilibrium nozzle flow,'' M.S. thesis, Dept. Electron. Eng., Osaka Univ., Osaka, Japan, 1993.
% \bibitem{c20} J. P. Wilkinson, ``Nonlinear resonant circuit devices (Patent style),'' U.S. Patent 3 624 12, July 16, 1990. 

% \end{thebibliography}




\end{document}
