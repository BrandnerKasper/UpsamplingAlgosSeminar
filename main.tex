%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
% ref packages
%\usepackage{nameref}
%\usepackage{varioref}
% \usepackage{hyperref}
\usepackage[hyphens,spaces,obeyspaces]{url} % Fixes long URLs
\usepackage{hyperref} % Let URLs be clickable and lead to the website

% \usepackage{enumitem} % Let enumerations start with the alphabet
\usepackage{graphicx}
\usepackage{floatrow}
\usepackage{float}
\usepackage{aliascnt}
\usepackage{multirow} % Tables
\usepackage{array}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\usepackage{authblk} % For the author section 
\usepackage{caption} % advanced captions
%Mathebib
\RequirePackage{amsmath}
\RequirePackage{amssymb}
\usepackage[
style=ieee, % Zitierstil
isbn=false,
doi=false,
url=false,
pagetracker=true,
%autocite=inline,  % regelt Aussehen für \autocite (inline=\parancite)
block=space,               
backref=false,
backrefstyle=three+,
date=year,
backend=biber
]{biblatex}
\addbibresource{literature.bib}
\renewcommand*{\bibfont}{\small}

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\title{\LARGE \bf
Current Trends in Games Technologies: Upsampling algorithms
}

\author{Tobias Brandner}

\affil{Julius-Maximilians University \\
        Würzburg, Germany \\
        tobias.brandner@stud-mail.uni-wuerzburg.de}

%\author{Huibert Kwakernaak$^{1}$ and Pradeep Misra$^{2}$% <-this % stops a space
%\thanks{*This work was not supported by any organization}% <-this % stops a space
%\thanks{$^{1}$H. Kwakernaak is with Faculty of Electrical Engineering, Mathematics and Computer Science,
    %    University of Twente, 7500 AE Enschede, The Netherlands
     %   {\tt\small h.kwakernaak at papercept.net}}%
%\thanks{$^{2}$P. Misra is with the Department of Electrical Engineering, Wright State University,
      %  Dayton, OH 45435, USA
       % {\tt\small p.misra at ieee.org}}%
%}


\begin{document}
\maketitle
\thispagestyle{empty}
\pagestyle{empty}

\BiblatexSplitbibDefernumbersWarningOff

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

sth with upsampling algos...

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

As games become more sophisticated by incorporating computational demanding features such as ray tracing, a method of calculating illumination by tracking virtual photons, 
which are reflected and scattered by surfaces \cite{ray_tracing}, the need for image upsampling technologies is increasing.
Deep Learning Super Sampling (DLSS) \cite{dlss} and FidelityFX Super Resolution (FSR) \cite{fsr} are two different methods to reduce computation cost 
by rendering the game in a lower resolution and later upsample them to the higher target resolution. 
While FSR uses traditional methods, DLSS uses machine learning approaches to achieve upsampling.\\
The game Cyperpunk \cite{Cyperpunk}, for example, recently received an experimental feature that has been developed in cooperation with Nvidia\footnote{\url{https://www.nvidia.com/de-de/}} 
called Ray Tracing: Overdrive \cite{ray_tracing_overdrive}, a path tracer that uses all light sources in the scene to calculate reflections for the current rendered frame \cite{Shirley2020RTW1}.
The calculation for this type of ray tracing is so demanding that an RTX 4090, 
one of the most powerful General Processing Units (GPU) \cite{4k_gpu_bench}, renders the game in 4k resolution at only 18 frames per second (fps) at average. 
This compares to an average of 48 fps without ray tracing. Using DLSS 2 the average fps increases to 59 fps, and with NVidia's latest upsampling technology DLSS 3 an average of 95 fps is achieved \cite{digital_foundry_ray}. 
A frame rate that makes the game fluid and responsive to play \cite{goodFPS}.
On the other hand, upsampling technologies can also be used to support weaker hardware such as older GPUs or mobile hardware \cite{dong2022rendersr}.
The Legend of Zelda: Tears of the Kingdom \cite{ZeldaTotK}, for example, is rendered natively in 720p resolution on the Nintendo Switch\footnote{\url{https://www.nintendo.com/switch/}} when played in handheld mode, 
but when docked to the station it is rendered in btw. 720p and 900p at 30 fps and further upscaled with FSR 1.0 to reach 1080p \cite{digital_foundry_zelda_fsr}. \\
The following section \ref{Sec:RelatedWork} explains the core ideas behind traditional and machine upsampling techniques.
Section \ref{Sec:Current} examines the state of the art of upsampling technologies used in games.
Section \ref{Sec:Evaluation} evaluates these upsampling technologies against some benchmarks, including recent game titles and their respective image quality.
Finally, section \ref{Sec:ConcFuture} summarizes the results and explores future trends.

\section{Background}
\label{Sec:RelatedWork}
Image Upsampling/Upscaling is the process of of creating a high resolution (HR) image from a low resolution (LR) image.
This section explains some of the most common ideas, as seen in figure \ref{fig:upsampling_methods}, when it comes to upsampling \cite{wang2022interpolation}\cite{wang2020deep}.
Starting with more traditional linear interpolation methods to cutting edge deep learning approaches. 

\begin{figure}[!ht]
        \caption{Hierachically-structured taxonomy of this paper.
        We review two methods for image upsampling: 
        Interpolation, e.g the linear Bicubic interpolation, and Deep Learning, consisting of supervised and unsupervised network models.}
        \centering
        \includegraphics[width=0.7\textwidth]{images/interpol.jpg}
        \label{fig:upsampling_methods}
    \end{figure}

\subsection{Interpolation}
\label{subsec:traditional}
Interpolation is the method of constructing new data points within the range of a discrete set of known data points.
In the context of images, this means that we need to "guess" the color information of the newly created pixels \cite{image_interpolation_def}.
One example: If we have an LR image of 720p (1280x720) and we want to double the resolution to 1440p (2560x1440), 
we need to create a new pixel for each pixel of our input image and interpolate their color values.

\subsubsection{Linear Interpolation}

Nearest neighbor interpolation is one of the simplest and fastest interpolation algorithms, but it provides poor image quality. 
The idea is that each interpolated pixel gets the same color value as its nearest neighbor. 
Therefore we need to identify the interpolated pixel's coordinate and scale it based on the ratio between the original image size and the target image size.
(coming back to our example 720p to 1440p, it would be 0.5) and chose the pixel in the original image with the nearest coordinate \cite{nearest_n}.
Bi-linear interpolation, as the name suggests, goes one step further and interpolates in two directions. 
Thus, the color values of the 4 nearest neighbors are taken and their average is used \cite{bilinear}.
Bicubic interpolation is the most common and computationally intensive linear method we discuss. 
It takes a 4x4 grid of neighboring pixels and weights them based on their distance (closer pixel values are weighted higher) \cite{bicubic}. 
Resulting in better image quality overall, except for some edge cases (pun intended).

\subsubsection{Nonlinear Interpolation}

Linear interpolation methods work well for upsampling pixel areas, but fail at edges, resulting in blurring.
For these cases, nonlinear interpolation methods such as unsharp masking can be used. Unsharp masking creates an out-of-focus version of the original image and subtracts it from the original, creating an image that contains only the edges.
This image can then be used to resharpen the edges of the upsampled image \cite{deng2010generalized}.
Another method is Laplacian sharpening, which uses the Laplacian operator to highlight areas of rapid intensity change (edges), and then reapplies it to the image, again resulting in sharper edges \cite{wang2022interpolation}.\\
Linear and nonlinear interpolation methods can be combined, as seen in figure \ref{fig:edge_det}, by detecting when to use a no-/linear method to obtain a high quality sampled image.

\begin{figure}[!ht]
        \caption{When upsampling an image from a low-resolution image, edges are detected to determine which type of interpolation method to use.
                If an edge is detected, a nonlinear interpolation method is used to sharpen the edge of the generated high-resolution image. 
                Otherwise, a linear interpolation method is used to fill areas in the generated image.}
        \centering
        \includegraphics[width=0.6\textwidth]{images/edgedet.jpg}
        \label{fig:edge_det}
    \end{figure}

\subsection{Deep Learning}

%Supervised models (LR images and corresponding HR images for training) vs Unsupervised \\
Image Super Resolution (SR) is a special form of image upsampling where more complex and advanced algorithms, e.g. deep neural network models, are used to generate an HR image from an LR image. 
In general, each neural network processes an input, in our case an LR image, with multiple layers (the more layers, the deeper the network) and produces an output, in our case an HR image.
The input, i.e. the image, is first converted into a tensor, a multidimensional array consisting of numbers representing the color values of each pixel. 
The layers then perform various mathematical operations based on their own parameters, i.e. weights to learn features like detecting pixel corresponding to edges.
The output of the network produces a loss that shows how well the network performed its task. 
This loss is then backprobagated to the various layers to update their parameters and adjust their weights to hopefully produce a better/more accurate output.
This process is repeated many times and is called training of the network \cite{goodfellow2016deep}.
After the network is trained, it can be used to generate HR images.\\
Since this is a very complex topic and there are many different neural network architectures, we will focus on some key architectures.

\subsubsection{A supervised method}
Supervised neural networks are networks whose input and output are carefully prepared and labeled, 
i.e., each LR image that is fed into the network has a corresponding HR image against which the output of the network can be compared.
%CNN erklären
The most common supervised neural network is the Covolution Neural Network (CNN).
It uses convolutional layers to extract features from training data. 
Convolutional layers perform a mathematical operation called convolution, which multiplies two higher-dimensional arrays to compute a new value (effectively downsampling our image).
The inverse of this operation, deconvolution or transposed convolution, can be used to sample the image back up.
\textcite{Gao2020PixelTC} has shown that their network PixelTCL can effectively overcome checkerboard artifacts, resulting in better spatial features such as edges and shapes.
\\
% GAN erklären
Another model is the Generative Adversial Network (GAN), which consists of two main components: a generator and a discriminator.
Both are trained simultaneously: 
The generator has the task of generating an image that the discriminator takes as input.
The discriminator receives a mixture of real and fake images from the generator, and its task is to discriminate between them.
GAN models have proven to be very successful when it comes to generating HR images \cite{Sharma2020HighresolutionID}.


\subsubsection{A Unsupervised method}
% UNet erkläten
% Figure größer machen -> maybe think about making it go over both columns
\begin{figure*}[!ht]
    \caption{An example of a UNet network architecture. 
    The input gets downsampled with intermediate steps and then again upsampled, forming a U-like structure.}
    \centering
    \includegraphics[width=0.8\textwidth]{images/U-NetExample.png}
    \label{fig:unet}
\end{figure*}
The U-Net architecture can be used for both supervised and unsupervised training. 
In this architecture, first the input is downsampled and then later upsampled with intermediate steps, creating a U-like structure, as seen in figure \ref{fig:unet}.
The unique feature of this network architecture is that each upsampling step is influenced not only by the previous step, but also by the same layer, so that more unique features can be learned \cite{Sharma2022ADL}. \\
The Convolutional Auto-Encoder is another unsupervised method. 
An autoencoder is a type of neural network that aims to learn a compressed representation or encoding of the input data.
It consists of an encoder network that maps the input image to a lower dimensional representation, 
a bottleneck layer that helps compress the extracted features into a smaller vector representation, 
and a decoder that gradually converts the compressed image into an upsampled image.
In the case of a convolutional autoencoder, the encoder and decoder networks consist of convolutional layers instead of fully connected layers \cite{conv_autoE}.
Convolutional autoencoders have proven effective in capturing spatial information and extracting features from images for use in image upsampling \cite{liu2021variational}.

\subsection{Anti-aliasing}

Anti-aliasing is a technique for eliminating the aliasing effect. 
Aliasing describes the appearance of stair-stepped lines or jagged edges in a rasterized image (image rendered with pixels), as seen in figure \ref{fig:antiAliasing}.
Aliasing typically occurs when displaying HR images on a lower resolution system, for example when smooth and continuous curves are rasterized \cite{antialiasing_def}.

\begin{figure}[!ht]
        \caption{An example of a) anti-aliasing having sharp edges, and b) aliasing having stair-stepped lines.}
        \centering
        \includegraphics[width=1.0\textwidth]{images/aliasing.jpg}
        \label{fig:antiAliasing}
    \end{figure}

The idea behind anti-aliasing is very similar to upsampling methods, only their use case differs, as interpolation is applied from LR images to HR images, while anti-aliasing is applied from HR images to LR images.
Anti-aliasing is usually needed for a large screen size with a relatively low screen resolution.\\
An example would be Super Sampling Anti-Aliasing (SSAA), which renders and downsamples the image at a higher resolution to produce a clearer and sharper image. 
Or Multi-Sample Anti-Aliasing (MSAA), which takes multiple samples per pixel during the rendering process. 
Instead of rendering a single color for each pixel, MSAA calculates the average color value based on the samples within the pixel range. 
By averaging multiple samples, MSAA can smooth edges and reduce the jagged appearance.
But these approaches are quite computationally expensive so using Morphological Anti-Aliasing (MLAA), a rule based pixel blending anti-aliasing method which added blurring to the image, gained a lot of popularity. 
Born from MLAA was Fast Approximate Anit-Aliasing (FAA) and Subpixel Methodological Anti-Aliasing (SMAA), which reduce the blur to a minimum, resulting in a overall better and sharper image.
The last Anti-Aliasing method is Temporal Anti-Aliasing (TAA), a complex and intensive method that uses a temporal component (frames that were rendered before the current generated image). 
TAA helps in smoothing artifacts from rash movement \cite{antialiasing_types} \cite{gu2022super} \cite{stuttgart_boy}.\\
In the context of video games, upsampling and anti-aliasing are often combined to provide a smooth and good-looking visual experience.

\subsection{Quality Metrics}

Measuring the quality of created images is not a trivial task, as the human eye is very perceptive when it comes to detecting subtle differences that make images realistic or unrealistic.
The most common objective quality measurement is the peak signal-to-noise ratio (PSNR). 
It compares two images pixel for pixel by calculating the mean squared error (MSE) between them, summing them up, and putting it on a logarithmic scale, resulting in a comparable value.
Because PSNR only considers the MSE at the pixel level, i.e., the differences between the corresponding pixels, and not the visual perception, it is often a poor representation of the overall quality of the reconstruction.
Structural similarity (SSIM) attempts to compensate for this shortcoming by measuring the structural similarity between two images. 
This is based on the assumption that the human eye is more sensitive to changes in image structures.
SSIM measures luminance, contrast, and structure independently by estimating luminance and contrast as the mean and image intensity as the standard deviation.
They are combined in pixel windows and summed to calculate a final metric value. \\
Both objective metrics may not indicate good overall image quality. Therefore, a subjective metric, such as Mean Opinion Score (MOS), may also be considered.
MOS allows test subjects to rate actual image quality, usually by assigning a simple score from 1 (poor) to 5 (good).
The MOS score is calculated by taking the average of all the scores. 
The MOS score is inferior to the objective scores when it comes to comparability, but can always serve as a good basis when objective measures provide varying results \cite{wang2020deep} \cite{Soufi2022BenchmarkOD}.


\section{Current Upsampling Technologies}
\label{Sec:Current}

\begin{table*}[ht]
    \centering
    \caption{FSR, DLSS, and XeSS have different quality modes that can be set. These upsampling modes usually include:
        a) A very high setting (best image quality with a scaling factor of 1.3).
        b) A high setting (good overall image quality with a scaling factor of 1.5).
        c) A medium setting (mixed image quality with a scaling factor of 1.7).
        d) A low setting (mediocre image quality with a scaling factor of 2.0).
    The lower the scaling factor, the more computationally intensive upscaling becomes!}
    \label{tab:qualityModes}
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Quality Mode} & \textbf{Scale Factor} & \textbf{Input Resolution} & \textbf{Output Resolution} \\
    \hline
    Very High   & 1.3x                          & 1477 x 831  & 1920 x 1080 (FHD) \\
                & (77\% screen resolution)      & 1970 x 1108 & 2560 x 1440 (WQHD) \\
                &                               & 2954 x 1662 & 3840 x 2160 (4k) \\
    \hline
    High        & 1.5x                          & 1280 x 720  & 1920 x 1080 \\
                & (67\% screen resolution)      & 1706 x 960  & 2560 x 1440 \\
                &                               & 2560 x 1440 & 3840 x 2160 \\
    \hline
    Medium      & 1.7x                          & 1129 x 635  & 1920 x 1080 \\
                & (59\% screen resolution)      & 1506 x 847  & 2560 x 1440 \\
                &                               & 2259 x 1270 & 3840 x 2160 \\
    \hline
    Low         & 2.0x                          & 960 x 540   & 1920 x 1080 \\
                & (50\% screen resolution)      & 1280 x 720  & 2560 x 1440 \\
                &                               & 1920 x 1080 & 3840 x 2160 \\
    \hline
    \end{tabular}
\end{table*}

So far, we have only dealt with the basics of upsampling techniques, which mostly focused on a static context (we did not care about how long it takes our algorithm to generate the HR image).
We will now focus on the latest upsampling technologies used in modern games. Therefore, these algorithms need to upsample our image in a real-time context, e.g. upsample an image 60 times per second.
In particular, we study FSR from AMD and DLSS from Nvidia, 
Xe Super Sampling (XeSS) \cite{XeSS_intro} from Intel\footnote{\url{https://www.intel.com/content/www/us/en/homepage.html}} 
and Temporal Super Resolution (TSR) \cite{TSR_intro} from Epic Games\footnote{\url{https://www.epicgames.com/site/en-US/home/}} (which is used exclusively in the Unreal Engine\footnote{\url{https://www.unrealengine.com/en-US/}}). \\
All of these technologies typically use 4 different quality modes to upscale the LR image, as seen in table \ref{tab:qualityModes}. 
For example, the very high quality mode uses a factor of 1.3 to scale the input resolution from 2954 x 1662 to 3840 x 2160 (here only 77\% of the output resolution is actually rendered by the game).
Higher scaling factors result in worse image quality, but increase the performance rate of the respective upsampling algorithm.
All quality modes target output resolutions of FHD (1080p), WQHD (1440p), and 4k (2160p).
To include an upsampling technology inside the render process of a game engine, like Unreal or Unity\footnote{\url{https://unity.com/}}, 
the upsampling pass needs to be applied after the anti-aliasing, but before rendering post processing effects, especially those that add noise like film grain, and HUD elements, as seen in image \ref{fig:renderpass}.

\begin{figure}[!ht]
    \caption{To include an upsampling technique in the render pass of a game engine, 
    it should be used after anti-aliasing the image, but before rendering post process effects, 
    like film grain or chromatic abberation, and the HUD elements.}
    \centering
    \includegraphics[width=0.3\textwidth]{images/renderpass.jpg}
    \label{fig:renderpass}
\end{figure}

\subsection{FidelityFX Super Resolution (FSR)}

FSR is AMD's open source, cross platform (runs on all brands of GPUs) upsampling technology. It was first introduced as FSR 1.0 in June 2021.
FSR 1.0\footnote{\url{https://github.com/GPUOpen-Effects/FidelityFX-FSR}} only relies on traditional interpolation methods. In particular it uses two passes: 
\begin{enumerate}
    \item An upscaling pass called Edge-Adaptive Spatial Upsampling (EASU) that interpolates and analyses the input frame to detect gradient reverseals (essentially looking at how strongly neighboring pixel values differ). 
    \item A sharpening pass called Robust Contrast-Adaptive Sharpening (RCAS) to reapply the detected edges into the upsampled image. 
\end{enumerate}

For FSR 1.0 to work well, it needs anti-aliasing (in form of TAA or MSAA) as well as a negative Multum In Parvo (latin for many things in a small space) (MIP) mapping bias.
MIP maps are precomputed versions of textures at different resolutions, they are used to improve rendering performance and reduce visual artifacts, like aliasing, by chosing the appropriate level of detail (LOD) of a texture for a given pixel \cite{MIP_map}.
The game engine automatically selects the MIP mapping. With a bias we can change the LOD of the texture. While a positive bias leads to a lower LOD and makes the texture appear blurry, a negative bias leads to a higher LOD and makes the texture sharper \cite{MIP_mapBias}.
In particular for FSR, different MIP biases are chosen depending on the quality level (-0.3 for very high to -1.0 for low).
As mentioned before, FSR needs to perform the upsampling in real time and therefore it is important to know how long the upsampling process takes.
Depending on the target resolution and GPU, upsampling takes less than 1 millisecond (ms) (4k with a 3080 takes 0.4 ms, with a 2060S it takes 1.0 ms).
For comparison, at a refresh rate of 60 fps, an image takes 16.6 ms to render. Therefore, the performance overhead of <=1ms is acceptable with FSR \cite{fsr1.0}.
Games can be either GPU-limited or CPU-limited, meaning that the game's frame rate depends either on how long it takes the game to render a frame (GPU limit) or how long it takes the game to compute a step in its logic (CPU limit) \cite{gpu-cpu-limit}.
Since upsampling techniques reduce the performance cost to the GPU, meaning when the game is GPU limited the performance overhead of the upsampling process when rendering the game at a lower resolution is far less than the overhead when rendering the game at a higher resolution, which results in a dramatic improvement in our frame rate!\\
FSR 2.x\footnote{\url{https://github.com/GPUOpen-Effects/FidelityFX-FSR2}} is AMD's latest upsampling technology and was released in May 2022.
It builds on FSR 1.0 and uses a temporal algorithm similar to TAA. The idea is to use motion vectors that describe how a game object moved from the previous frame to the current one.
This history data can then be used to increase the accuracy of the interpolated pixels by calculating where each pixel in the previous frame moved to.
In cases where the history data no longer correlates with the new input frame, temporal ghosting (a visual artifact best seen in fast-moving objects that trace themselves) occurs. 
Since FSR 2.x already uses something similar to TAA, an anti-aliasing image as input is not strictly necessary (the MIP bias remains the same).
Therefore, it is more computational demanding, which leads to a doubling of the performance overhead (4k with a RX 6800XT takes 1.1ms) \cite{fsr2.x}.\\
FSR 1.0 is integrated with Godot \cite{fsr1_godot} and Unity \cite{fsr1_unity}. While Unreal supports it through a plugin \cite{fsr1_unreal}.
At the moment only Unreal supports FSR 2.x via a plugin \cite{fsr2.x_unreal}.

\subsection{Deep Learning Super Sampling (DLSS)}

DLSS is Nvidia's proprietary deep learning upsampling technology that only works on the company's RTX GPUs.
First introduced in 2018, it uses TAA for the LR image and then feeds it into a convolutional autoencoder network to get an upsampled HR image.
The network is trained by comparing its output to an even higher resolution image, e.g. the network upscales a 1080p image to 4k and compares it to a 16k base image \cite{dlss_slide_1}.
The network can then be fine-tuned by optimizing for several losses, such as content loss and style loss.
The content loss measures the difference between the content of the base image and the up-sampled image.
It ensures that important content and structural details of the original image are captured.
Content loss is usually calculated using a distance metric such as MSE.
Style loss, also known as texture loss, focuses on capturing the style or artistic features of the base image.
Style is often represented by correlations of different feature vectors in the CNN, which are stored in the Gram matrix.
Style loss is calculated by comparing the Gram matrices of the feature vectors of the original image and the upscaled image \cite{liu2021variational}.\\
In 2020, Nvidia further improved DLSS to DLSS 2 by adding information about previous images in the form of motion vectors. 
This temporal feedback is then fed into the network in addition to the TAA input image to further improve the network's output.
When integrating DLSS 2, it is recommended to use a MIP mapping bias based on the formula \ref{dlss_mip_bias} \cite{dlss_slide_2}.

\begin{equation}
    \log_2\left(\frac{\text{{renderWidth}}}{\text{{outputWidth}}}\right) - 1 + \text{{eps}}
    \label{dlss_mip_bias}
\end{equation}

A look at the official SDK\footnote{\url{https://github.com/NVIDIA/DLSS}} of DLSS 2 shows that 6 different presets can be selected, which means that there are 6 different configurations for the parameters of the network. \\
DLSS 3 is Nvidia's latest upsampling technology, introduced in 2022, and only works on Generation 4000 GPUs.
Building on DLSS 2, DLSS 3 adds optical frame generation that not only allows an LR frame to be upsampled to an HR frame, but also creates an entirely new frame between the upsampled frames.
The DLSS 3 Convolutional Autoencoder requires four inputs - current and previous frames, an optical flow field, and game engine data such as motion vectors and depths.
The optical flow field is calculated by analyzing two consecutive frames in the game.
This makes it possible to track history data not only for game objects, as motion vectors do, but also for pixel-level information such as particles, reflections, shadows, and lighting.
Because DLSS 3 generates an entire fake frame between real frames (for which no game logic calculations are required), it reduces the performance overhead for the CPU.
When a game is CPU-limited, this behavior leads to an improvement in our frame rate!
While the fake frames are displayed, the game does not respond to the player's input.
This reduces the overall responsiveness of the game.
(Note: To solve this problem, Nvidia has introduced another technology called Refelex).
The high-definition rendering pipeline in Unity natively supports DLSS 2 \cite{dlss_unity}.
Unreal supports both DLSS 2 and DLSS 3 as a plugin \cite{dlss_unreal}.

\subsection{Xe Super Sampling (XeSS)}
\subsection{Temporal Super Resolution (TSR)}

\section{Evaluation}
\label{Sec:Evaluation}
\subsection{Benchmarks}
\subsection{Image Quality}

\section{Conclusion \& Future Work}
\label{Sec:ConcFuture}


% \addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\printbibliography[
title={Papers},
notkeyword=game, notkeyword=online
]

\printbibliography[
title={Online},
keyword=online
]

\printbibliography[
title={Ludography},
keyword=game
]
%\printbibliography

% \begin{thebibliography}{99}

% \bibitem{c1} G. O. Young, ``Synthetic structure of industrial plastics (Book style with paper title and editor),'' 	in Plastics, 2nd ed. vol. 3, J. Peters, Ed.  New York: McGraw-Hill, 1964, pp. 15--64.
% \bibitem{c2} W.-K. Chen, Linear Networks and Systems (Book style).	Belmont, CA: Wadsworth, 1993, pp. 123--135.
% \bibitem{c3} H. Poor, An Introduction to Signal Detection and Estimation.   New York: Springer-Verlag, 1985, ch. 4.
% \bibitem{c4} B. Smith, ``An approach to graphs of linear forms (Unpublished work style),'' unpublished.
% \bibitem{c5} E. H. Miller, ``A note on reflector arrays (Periodical styleÑAccepted for publication),'' IEEE Trans. Antennas Propagat., to be publised.
% \bibitem{c6} J. Wang, ``Fundamentals of erbium-doped fiber amplifiers arrays (Periodical styleÑSubmitted for publication),'' IEEE J. Quantum Electron., submitted for publication.
% \bibitem{c7} C. J. Kaufman, Rocky Mountain Research Lab., Boulder, CO, private communication, May 1995.
% \bibitem{c8} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interfaces(Translation Journals style),'' IEEE Transl. J. Magn.Jpn., vol. 2, Aug. 1987, pp. 740--741 [Dig. 9th Annu. Conf. Magnetics Japan, 1982, p. 301].
% \bibitem{c9} M. Young, The Techincal Writers Handbook.  Mill Valley, CA: University Science, 1989.
% \bibitem{c10} J. U. Duncombe, ``Infrared navigationÑPart I: An assessment of feasibility (Periodical style),'' IEEE Trans. Electron Devices, vol. ED-11, pp. 34--39, Jan. 1959.
% \bibitem{c11} S. Chen, B. Mulgrew, and P. M. Grant, ``A clustering technique for digital communications channel equalization using radial basis function networks,'' IEEE Trans. Neural Networks, vol. 4, pp. 570--578, July 1993.
% \bibitem{c12} R. W. Lucky, ``Automatic equalization for digital communication,'' Bell Syst. Tech. J., vol. 44, no. 4, pp. 547--588, Apr. 1965.
% \bibitem{c13} S. P. Bingulac, ``On the compatibility of adaptive controllers (Published Conference Proceedings style),'' in Proc. 4th Annu. Allerton Conf. Circuits and Systems Theory, New York, 1994, pp. 8--16.
% \bibitem{c14} G. R. Faulhaber, ``Design of service systems with priority reservation,'' in Conf. Rec. 1995 IEEE Int. Conf. Communications, pp. 3--8.
% \bibitem{c15} W. D. Doyle, ``Magnetization reversal in films with biaxial anisotropy,'' in 1987 Proc. INTERMAG Conf., pp. 2.2-1--2.2-6.
% \bibitem{c16} G. W. Juette and L. E. Zeffanella, ``Radio noise currents n short sections on bundle conductors (Presented Conference Paper style),'' presented at the IEEE Summer power Meeting, Dallas, TX, June 22--27, 1990, Paper 90 SM 690-0 PWRS.
% \bibitem{c17} J. G. Kreifeldt, ``An analysis of surface-detected EMG as an amplitude-modulated noise,'' presented at the 1989 Int. Conf. Medicine and Biological Engineering, Chicago, IL.
% \bibitem{c18} J. Williams, ``Narrow-band analyzer (Thesis or Dissertation style),'' Ph.D. dissertation, Dept. Elect. Eng., Harvard Univ., Cambridge, MA, 1993. 
% \bibitem{c19} N. Kawasaki, ``Parametric study of thermal and chemical nonequilibrium nozzle flow,'' M.S. thesis, Dept. Electron. Eng., Osaka Univ., Osaka, Japan, 1993.
% \bibitem{c20} J. P. Wilkinson, ``Nonlinear resonant circuit devices (Patent style),'' U.S. Patent 3 624 12, July 16, 1990. 

% \end{thebibliography}




\end{document}
